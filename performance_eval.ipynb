{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for performance evaluation of techniques"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We have four methods:**\n",
    "\n",
    "- Raw Attention (seems to be only mean of weights from input embeddings on first layer).\n",
    "- Attention flow (best in final layer).\n",
    "- Attention Rollout (best in final layer).\n",
    "- Gradient Attention Rollout (best in final layer (most likely)).\n",
    "\n",
    "**We have two possible methods for metric evaluation:**\n",
    "\n",
    "- Blank-out -> generate black spots and look at drop in accuracy.\n",
    "- Gradient input -> **(?)**.\n",
    "\n",
    "**Pipeline for evaluating models:**\n",
    "\n",
    "- Select $N$ images for study.\n",
    "- For each image (also can be done in multiple layers, but time constraint): \n",
    "    - Compute its raw attention, attention flow, rollout, and gradient rollout.\n",
    "    - Compute the blank-out and gradient input.\n",
    "    - Each will yield a vector of size $n_{embedding}$.\n",
    "    - Compute Spearman rank correlation between them and store it\n",
    "\n",
    "We should end up with a table of size $n_{images} \\times n_{methods}$. It suffices to compute mean and standard deviation for each method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import numpy\n",
    "import sys\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from vit_rollout import VITAttentionRollout\n",
    "from vit_grad_rollout import VITAttentionGradRollout\n",
    "from vit_flow import VITAttentionFlow\n",
    "\n",
    "import copy\n",
    "import requests\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('facebookresearch/deit:main', \n",
    "        'deit_tiny_patch16_224', pretrained=True)\n",
    "model.eval()\n",
    "model.to(DEVICE)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path, transform):\n",
    "    img = Image.open(image_path)\n",
    "    input_tensor = transform(img).unsqueeze(0)\n",
    "    return input_tensor.to(DEVICE)\n",
    "\n",
    "def get_prediction(scores):\n",
    "    '''Gets the index of max prob and the prob\n",
    "    '''\n",
    "    h_x = F.softmax(scores, dim=1).data.squeeze()\n",
    "    probs, idx = h_x.sort(0, True)\n",
    "    # output the prediction\n",
    "    return idx[0].item(), probs[0].item()\n",
    "\n",
    "# idx, prob = get_prediction(scores)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Attention Rollout masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_attention_rollout = 'attention_rollout.txt'\n",
    "file_attention_grad_rollout = 'attention_grad_rollout.txt'\n",
    "file_attention_flow = 'attention_flow.txt'\n",
    "\n",
    "with open(file_attention_rollout, 'w') as f:\n",
    "    pass  # Opening in 'w' mode clears the file\n",
    "\n",
    "with open(file_attention_grad_rollout, 'w') as f:\n",
    "    pass  # Opening in 'w' mode clears the file\n",
    "\n",
    "with open(file_attention_flow, 'w') as f:\n",
    "    pass  # Opening in 'w' mode clears the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_prefix = 'images/ILSVRC2012_val_00000'\n",
    "path_suffix = '.JPEG'\n",
    "discard_ratio = 0.9\n",
    "image_size = 224\n",
    "\n",
    "def convert_number(number):\n",
    "    if number < 10:\n",
    "        return '00'+str(number)\n",
    "    if number < 100:\n",
    "        return '0'+str(number)\n",
    "    else:\n",
    "        return str(number)\n",
    "\n",
    "# img = Image.open(path_prefix + image_number_converted + path_suffix)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current image being treated: 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n",
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n",
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n",
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n",
      "100%|██████████| 197/197 [10:26<00:00,  3.18s/it]\n",
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n",
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current image being treated: 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n",
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n",
      "100%|██████████| 197/197 [11:34<00:00,  3.52s/it]\n",
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n",
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current image being treated: 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n",
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n",
      "100%|██████████| 197/197 [10:10<00:00,  3.10s/it]\n",
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n",
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current image being treated: 38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n",
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n",
      "100%|██████████| 197/197 [12:48<00:00,  3.90s/it]\n",
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current image being treated: 39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n",
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n",
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n",
      "100%|██████████| 197/197 [17:56<00:00,  5.46s/it]  \n",
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n",
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current image being treated: 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n",
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n",
      "100%|██████████| 197/197 [10:33<00:00,  3.21s/it]\n",
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n",
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current image being treated: 41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n",
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n",
      "100%|██████████| 197/197 [10:36<00:00,  3.23s/it]\n",
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current image being treated: 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n",
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n",
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n",
      "100%|██████████| 197/197 [11:03<00:00,  3.37s/it]\n",
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n",
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current image being treated: 43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n",
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n",
      "100%|██████████| 197/197 [13:57<00:00,  4.25s/it]\n",
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current image being treated: 44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n",
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n",
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n",
      "100%|██████████| 197/197 [27:17<00:00,  8.31s/it]\n",
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current image being treated: 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n",
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n",
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n",
      "100%|██████████| 197/197 [15:31<00:00,  4.73s/it]\n",
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current image being treated: 46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n",
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n",
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n",
      "100%|██████████| 197/197 [29:32<00:00,  9.00s/it]\n",
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n",
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current image being treated: 47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n",
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n",
      "100%|██████████| 197/197 [06:01<00:00,  1.84s/it]\n",
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current image being treated: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n",
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n",
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n",
      "100%|██████████| 197/197 [06:11<00:00,  1.88s/it]\n",
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n",
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current image being treated: 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n",
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n",
      "100%|██████████| 197/197 [06:03<00:00,  1.85s/it]\n",
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n",
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current image being treated: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n",
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n",
      "100%|██████████| 197/197 [16:02<00:00,  4.88s/it]   \n",
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n",
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current image being treated: 51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n",
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n",
      "100%|██████████| 197/197 [10:51<00:00,  3.31s/it]\n",
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n",
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current image being treated: 52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n",
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n",
      "100%|██████████| 197/197 [11:05<00:00,  3.38s/it]\n",
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n",
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current image being treated: 53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n",
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n",
      "100%|██████████| 197/197 [18:48<00:00,  5.73s/it] \n",
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n",
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current image being treated: 54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n",
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n",
      "100%|██████████| 197/197 [10:27<00:00,  3.18s/it]\n",
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n",
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current image being treated: 55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n",
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n",
      "100%|██████████| 197/197 [10:29<00:00,  3.20s/it]\n",
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current image being treated: 56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n",
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n",
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n",
      "100%|██████████| 197/197 [08:23<00:00,  2.56s/it]\n",
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n",
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current image being treated: 57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n",
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n",
      "100%|██████████| 197/197 [08:08<00:00,  2.48s/it]\n",
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n",
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current image being treated: 58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n",
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n",
      "100%|██████████| 197/197 [06:01<00:00,  1.84s/it]\n",
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n",
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current image being treated: 59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n",
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n",
      "100%|██████████| 197/197 [06:01<00:00,  1.84s/it]\n",
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n",
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current image being treated: 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n",
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n",
      "100%|██████████| 197/197 [06:01<00:00,  1.84s/it]\n"
     ]
    }
   ],
   "source": [
    "for image_number in range(35,61):\n",
    "    print(f\"Current image being treated: {image_number}\")\n",
    "\n",
    "    image_number_converted = convert_number(image_number)\n",
    "    image_path = path_prefix + image_number_converted + path_suffix\n",
    "    input_tensor  = preprocess_image(image_path, transform)\n",
    "\n",
    "    # Getting idx\n",
    "    model = torch.hub.load('facebookresearch/deit:main', 'deit_tiny_patch16_224', pretrained=True)\n",
    "    model.eval()\n",
    "    model.to(DEVICE)\n",
    "    scores = model(input_tensor)\n",
    "    category_index, _ = get_prediction(scores)\n",
    "\n",
    "    # Attention Rollout\n",
    "    model = torch.hub.load('facebookresearch/deit:main', 'deit_tiny_patch16_224', pretrained=True)\n",
    "    model.eval()\n",
    "    attention_rollout = VITAttentionRollout(model, discard_ratio=discard_ratio)\n",
    "    mask_attention_rollout = attention_rollout.get_attention_mask(input_tensor).numpy()\n",
    "    mask_attention_rollout = mask_attention_rollout / np.max(mask_attention_rollout)\n",
    "    \n",
    "    with open(file_attention_rollout, 'a') as f:\n",
    "        np.savetxt(f, [mask_attention_rollout], fmt='%.3f', delimiter=',')  # Saving as float\n",
    "\n",
    "    # Gradient Attention Rollout\n",
    "    model = torch.hub.load('facebookresearch/deit:main', 'deit_tiny_patch16_224', pretrained=True)\n",
    "    model.eval()\n",
    "    attention_grad_rollout = VITAttentionGradRollout(model, discard_ratio=discard_ratio)\n",
    "    mask_attention_grad_rollout = attention_grad_rollout.get_attention_mask(input_tensor, category_index=category_index).numpy()\n",
    "    mask_attention_grad_rollout = mask_attention_grad_rollout / np.max(mask_attention_grad_rollout)\n",
    "    \n",
    "    with open(file_attention_grad_rollout, 'a') as f:\n",
    "        np.savetxt(f, [mask_attention_grad_rollout], fmt='%.3f', delimiter=',')  # Adjust format as needed\n",
    "\n",
    "    # Attention Flow\n",
    "    model = torch.hub.load('facebookresearch/deit:main', 'deit_tiny_patch16_224', pretrained=True)\n",
    "    model.eval()\n",
    "    attention_flow = VITAttentionFlow(model, discard_ratio=discard_ratio)\n",
    "    mask_attention_flow = attention_flow.get_attention_mask(input_tensor).numpy()\n",
    "    mask_attention_flow = mask_attention_flow / np.max(mask_attention_flow)\n",
    "    \n",
    "    with open(file_attention_flow, 'a') as f:\n",
    "        np.savetxt(f, [mask_attention_flow], fmt='%.3f', delimiter=',')  # Adjust format as needed\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first train_1_to-33\n",
    "#second 35 to end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# LABELS_URL = 'https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json'\n",
    "# classes = {int(key): value for (key, value) in requests.get(LABELS_URL).json().items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_tensor  = preprocess_image(\"examples/input.png\", transform)\n",
    "# scores = model(input_tensor)\n",
    "\n",
    "# def print_preds(scores):\n",
    "#     # print the predictions with their 'probabilities' from the scores\n",
    "#     h_x = F.softmax(scores, dim=1).data.squeeze()\n",
    "#     probs, idx = h_x.sort(0, True)\n",
    "#     probs = probs.numpy()\n",
    "#     idx = idx.numpy()\n",
    "#     # output the prediction\n",
    "#     for i in range(0, 5):\n",
    "#         print('{:.3f} -> {}'.format(probs[i], classes[idx[i]]))\n",
    "#     return idx\n",
    "\n",
    "# def get_prediction(scores):\n",
    "#     '''Gets the index of max prob and the prob\n",
    "#     '''\n",
    "#     h_x = F.softmax(scores, dim=1).data.squeeze()\n",
    "#     probs, idx = h_x.sort(0, True)\n",
    "#     # output the prediction\n",
    "#     return idx[0].item(), probs[0].item()\n",
    "\n",
    "# idx, prob = get_prediction(scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
