{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing flow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook for implementing the Flow algorithm in the Attention structure.\n",
    "\n",
    "Main idea: Given a model, assign as **source** its **class token** embedding at **final layer**. The **sinks** will be the **input tokens**. The Graph will be generated by the matrices $A$ as in the paper:\n",
    "\n",
    "$$\n",
    "A = \\frac{W_{att} + I}{2} \n",
    "$$\n",
    "\n",
    "Plus we have to renormalize (check how normalization is done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import numpy\n",
    "import sys\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import maxflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_a_matrices(attentions, discard_ratio, head_fusion):\n",
    "    '''Generates the A matrices as in the paper from the attention layers\n",
    "    '''\n",
    "    a_matrices = []\n",
    "    \n",
    "    for attention in attentions:\n",
    "        # Getting type of fusion of channels\n",
    "        if head_fusion == \"mean\":\n",
    "            attention_heads_fused = attention.mean(axis=1)\n",
    "        elif head_fusion == \"max\":\n",
    "            attention_heads_fused = attention.max(axis=1)[0]\n",
    "        elif head_fusion == \"min\":\n",
    "            attention_heads_fused = attention.min(axis=1)[0]\n",
    "        else:\n",
    "            raise \"Attention head fusion type Not supported\"\n",
    "        \n",
    "        # Drop the lowest attentions, but\n",
    "        # don't drop the class token\n",
    "        flat = attention_heads_fused.view(attention_heads_fused.size(0), -1)\n",
    "        _, indices = flat.topk(int(flat.size(-1)*discard_ratio), -1, False)\n",
    "        indices = indices[indices != 0]\n",
    "        flat[0, indices] = 0\n",
    "        \n",
    "        \n",
    "        I = torch.eye(attention_heads_fused.size(-1))\n",
    "        a = (attention_heads_fused + 1.0*I)/2\n",
    "        \n",
    "        a = a / a.sum(dim=-1)\n",
    "        # a = a / a.sum(dim=-1, keepdim=True) -> verify which one is correct\n",
    "        \n",
    "        a_matrices.append(a)  \n",
    "    \n",
    "    return a_matrices\n",
    "    \n",
    "    \n",
    "def make_graph(a_matrices):\n",
    "    '''Must make graph such the the source is the class token at final layer (or given layer)\n",
    "    and the sink are the input embeddings\n",
    "    Uses PyMaxflow (or not)\n",
    "    The output must be the weights assigned to the initial tokens...\n",
    "    '''\n",
    "    ...\n",
    "\n",
    "def flow(attentions, discard_ratio, head_fusion):\n",
    "    '''Generates attention flow mask in similar fashion as rollout\n",
    "    '''\n",
    "    \n",
    "    # Get the a_matrices\n",
    "    a_matrices = compute_a_matrices(attentions, discard_ratio, head_fusion)\n",
    "    \n",
    "    # Generates the graph structure\n",
    "    graph = make_graph(a_matrices)\n",
    "    \n",
    "    # must generate mask\n",
    "    mask = ... # TODO\n",
    "\n",
    "    # Adjust for output\n",
    "    width = int(mask.size(-1)**0.5)\n",
    "    mask = mask.reshape(width, width).numpy()\n",
    "    mask = mask / np.max(mask)\n",
    "    \n",
    "    return mask\n",
    "\n",
    "class VITAttentionFlow:\n",
    "    def __init__(self, model, attention_layer_name='attn_drop', head_fusion=\"mean\", discard_ratio=0.9):\n",
    "        self.model = model\n",
    "        self.head_fusion = head_fusion\n",
    "        self.discard_ratio = discard_ratio\n",
    "        \n",
    "        for name, module in self.model.named_modules():\n",
    "            if attention_layer_name in name:\n",
    "                module.register_forward_hook(self.get_attention)\n",
    "\n",
    "        self.attentions = []\n",
    "\n",
    "    def get_attention(self, module, input, output):\n",
    "        self.attentions.append(output.cpu())\n",
    "\n",
    "    def __call__(self, input_tensor):\n",
    "        self.attentions = []\n",
    "        with torch.no_grad():\n",
    "            output = self.model(input_tensor)\n",
    "\n",
    "        return flow(self.attentions, self.discard_ratio, self.head_fusion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
