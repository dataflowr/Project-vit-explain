{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mouha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import numpy\n",
    "import sys\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from vit_rollout import VITAttentionRollout\n",
    "from vit_grad_rollout import VITAttentionGradRollout\n",
    "from vit_flow import VITAttentionFlow\n",
    "import requests\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\mouha/.cache\\torch\\hub\\facebookresearch_deit_main\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 192, kernel_size=(16, 16), stride=(16, 16))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (norm_pre): Identity()\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (1): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (2): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (3): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (4): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (5): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (6): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (7): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (8): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (9): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (10): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (11): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "  (fc_norm): Identity()\n",
       "  (head): Linear(in_features=192, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = Image.open(\"examples/input.png\")\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {DEVICE}\")\n",
    "\n",
    "IMAGE_SIZE = 224\n",
    "DISCARD_RATIO = 0.9\n",
    "\n",
    "def preprocess_image(image_path, transform):\n",
    "    img = Image.open(image_path)\n",
    "    input_tensor = transform(img).unsqueeze(0)\n",
    "    return input_tensor.to(DEVICE)\n",
    "\n",
    "def show_mask_on_image(img, mask):\n",
    "    img = np.float32(img) / 255\n",
    "    heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET)\n",
    "    heatmap = np.float32(heatmap) / 255\n",
    "    cam = heatmap + np.float32(img)\n",
    "    cam = cam / np.max(cam)\n",
    "    return np.uint8(255 * cam)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "])\n",
    "\n",
    "model = torch.hub.load('facebookresearch/deit:main', \n",
    "        'deit_tiny_patch16_224', pretrained=True)\n",
    "model.eval()\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Select the rollout strategy to be used and display results\n",
    "#@markdown **Note** that for `grad_attention_rollout` passing a category index is mandatory.\n",
    "rollout_strategy = \"attention_flow\" #@param [\"attention_rollout\", \"grad_attention_rollout\"]\n",
    "category_index =  264#@param {type:\"integer\"}\n",
    "\n",
    "print(f\"Using {rollout_strategy}\")\n",
    "\n",
    "input_tensor  = preprocess_image(\"examples/input.png\", transform)\n",
    "\n",
    "if rollout_strategy == \"grad_attention_rollout\" and category_index < 0:\n",
    "    raise Exception(\"Category index is mandatory when using Gradient Attention Rollout\")\n",
    "\n",
    "elif rollout_strategy == \"grad_attention_rollout\" and category_index > 0:\n",
    "    grad_rollout = VITAttentionGradRollout(model, discard_ratio=DISCARD_RATIO)\n",
    "    mask = grad_rollout(input_tensor, category_index)\n",
    "    name = \"grad_rollout_{}_{:.3f}_{}.png\".format(category_index,\n",
    "        DISCARD_RATIO, \"mean\")\n",
    "\n",
    "elif rollout_strategy == \"attention_rollout\":\n",
    "    attention_rollout = VITAttentionRollout(model, discard_ratio=DISCARD_RATIO)\n",
    "    mask = attention_rollout(input_tensor)\n",
    "    name = \"attention_rollout_{:.3f}_{}.png\".format(DISCARD_RATIO, \"mean\")\n",
    "    \n",
    "elif rollout_strategy == \"attention_flow\":\n",
    "    attention_flow = VITAttentionFlow(model, discard_ratio=DISCARD_RATIO)\n",
    "    mask = attention_flow(input_tensor)\n",
    "    name = \"attention_flow_{:.3f}_{}.png\".format(DISCARD_RATIO, \"mean\")\n",
    "\n",
    "np_img = np.array(img)[:, :, ::-1]\n",
    "mask = cv2.resize(mask, (np_img.shape[1], np_img.shape[0]))\n",
    "mask = show_mask_on_image(np_img, mask)\n",
    "\n",
    "# mask = (mask - mask.min()) / (mask.max() - mask.min())\n",
    "# mask = mask.clip(0.7,1)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(16, 16))\n",
    "\n",
    "ax1.set_title('Original')\n",
    "ax2.set_title('Attention Map')\n",
    "_ = ax1.imshow(img)\n",
    "_ = ax2.imshow(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using attention_flow\n"
     ]
    }
   ],
   "source": [
    "img=Image.open(\"examples/plane.png\")\n",
    "#@title Select the rollout strategy to be used and display results\n",
    "#@markdown **Note** that for `grad_attention_rollout` passing a category index is mandatory.\n",
    "rollout_strategy = \"attention_flow\" #@param [\"attention_rollout\", \"grad_attention_rollout\"]\n",
    "category_index =  264#@param {type:\"integer\"}\n",
    "\n",
    "print(f\"Using {rollout_strategy}\")\n",
    "\n",
    "input_tensor  = preprocess_image(\"examples/plane.png\", transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if rollout_strategy == \"grad_attention_rollout\" and category_index < 0:\n",
    "    raise Exception(\"Category index is mandatory when using Gradient Attention Rollout\")\n",
    "\n",
    "elif rollout_strategy == \"grad_attention_rollout\" and category_index > 0:\n",
    "    grad_rollout = VITAttentionGradRollout(model, discard_ratio=DISCARD_RATIO)\n",
    "    mask = grad_rollout(input_tensor, category_index)\n",
    "    name = \"grad_rollout_{}_{:.3f}_{}.png\".format(category_index,\n",
    "        DISCARD_RATIO, \"mean\")\n",
    "\n",
    "elif rollout_strategy == \"attention_rollout\":\n",
    "    attention_rollout = VITAttentionRollout(model, discard_ratio=DISCARD_RATIO)\n",
    "    mask = attention_rollout(input_tensor)\n",
    "    name = \"attention_rollout_{:.3f}_{}.png\".format(DISCARD_RATIO, \"mean\")\n",
    "    \n",
    "elif rollout_strategy == \"attention_flow\":\n",
    "    attention_flow = VITAttentionFlow(model, discard_ratio=DISCARD_RATIO)\n",
    "    mask = attention_flow(input_tensor)\n",
    "    name = \"attention_flow_{:.3f}_{}.png\".format(DISCARD_RATIO, \"mean\")\n",
    "\n",
    "np_img = np.array(img)[:, :, ::-1]\n",
    "mask = cv2.resize(mask, (np_img.shape[1], np_img.shape[0]))\n",
    "mask = show_mask_on_image(np_img, mask)\n",
    "\n",
    "# mask = (mask - mask.min()) / (mask.max() - mask.min())\n",
    "# mask = mask.clip(0.7,1)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(16, 16))\n",
    "\n",
    "ax1.set_title('Original')\n",
    "ax2.set_title('Attention Map')\n",
    "_ = ax1.imshow(img)\n",
    "_ = ax2.imshow(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attacks on the model\n",
    "--------------------------------------------------------------------------------------------------------------------------------------\n",
    "-------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking simple predictions of the model\n",
    "LABELS_URL = 'https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json'\n",
    "classes = {int(key): value for (key, value) in requests.get(LABELS_URL).json().items()}\n",
    "\n",
    "scores = model(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['n02690373', 'airliner']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes[404]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.304 -> ['n02690373', 'airliner']\n",
      "0.084 -> ['n02692877', 'airship']\n",
      "0.057 -> ['n04592741', 'wing']\n",
      "0.055 -> ['n04552348', 'warplane']\n",
      "0.034 -> ['n04266014', 'space_shuttle']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def print_preds(scores):\n",
    "    # print the predictions with their 'probabilities' from the scores\n",
    "    scores=scores.cpu()\n",
    "    h_x = F.softmax(scores, dim=1).data.squeeze()\n",
    "    probs, idx = h_x.sort(0, True)\n",
    "    probs = probs.numpy()\n",
    "    idx = idx.numpy()\n",
    "    # output the prediction\n",
    "    for i in range(0, 5):\n",
    "        print('{:.3f} -> {}'.format(probs[i], classes[idx[i]]))\n",
    "    return idx\n",
    "\n",
    "_ = print_preds(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fast Gradient Sign Method Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#performing fgsm attack\n",
    "def fgsm_attack(image, epsilon, data_grad):\n",
    "    # Collect the element-wise sign of the data gradient\n",
    "    grad_sign = torch.sign(data_grad)\n",
    "\n",
    "    # Create the perturbed image by adjusting each pixel of the input image\n",
    "    perturbed_image = image + epsilon * grad_sign\n",
    "\n",
    "    # Adding clipping to maintain [0,1] range\n",
    "    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "\n",
    "    # Return the perturbed image\n",
    "    return perturbed_image\n",
    "\n",
    "idx = 656 #minivan\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "input_tensor.requires_grad = True\n",
    "scores = model(input_tensor)\n",
    "scores=scores.cpu()\n",
    "target = torch.tensor([idx])\n",
    "\n",
    "#TODO: compute the loss to backpropagate\n",
    "loss = criterion(scores, target)\n",
    "loss.backward()\n",
    "\n",
    "_ = print_preds(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Blur Attack: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "def gaussian_blur_attack(image, sigma=1):\n",
    "   \n",
    "    image_cpu = image.cpu().detach().numpy()\n",
    "    perturbed_image = gaussian_filter(image_cpu, sigma=sigma)\n",
    "  \n",
    "    perturbed_image = torch.tensor(perturbed_image).cuda()\n",
    "\n",
    "    return perturbed_image\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "x_att=gaussian_blur_attack(input_tensor, sigma=1)\n",
    "\n",
    "\n",
    "idx = 656 #minivan\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "input_tensor.requires_grad = True\n",
    "scores = model(input_tensor)\n",
    "scores=scores.cpu()\n",
    "target = torch.tensor([idx])\n",
    "\n",
    "#TODO: compute the loss to backpropagate\n",
    "loss = criterion(scores, target)\n",
    "loss.backward()\n",
    "\n",
    "_ = print_preds(scores)\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Massive adversaries on differents images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes sur les attaques\n",
    "\n",
    "attaquer les zones ou les attentions sont concentrées\n",
    "\n",
    "faire une boucle for et attaquer jusqu'a ce que la première prédiction change\n",
    "\n",
    "prendre une matrice de bruit.\n",
    "comparer par rapport aux méthodes\n",
    "\n",
    "faire un rank de quels blocs est plus susceptible aux attaques???\n",
    "\n",
    "\n",
    "fixer une zone initiale et faire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_prediction(scores):\n",
    "    scores=scores.cpu()\n",
    "    h_x = F.softmax(scores, dim=1).data.squeeze()\n",
    "    probs, idx = h_x.sort(0, True)\n",
    "    probs = probs.numpy()\n",
    "    idx = idx.numpy()\n",
    "    return  classes[idx[0]] #get the highest score\n",
    "\n",
    "def fgsm_attack(image, epsilon, data_grad):\n",
    "    grad_sign = torch.sign(data_grad)\n",
    "\n",
    "    perturbed_image = image + epsilon * grad_sign\n",
    "\n",
    "    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "\n",
    "    return perturbed_image\n",
    "eps=0.1\n",
    "def compute_one_attack(x_img,data_grad,epsilon=eps):\n",
    "    return fgsm_attack(x_img,epsilon,data_grad)\n",
    "    \n",
    "\n",
    "def preprocess_image(img):\n",
    "    input_tensor = transform(img).unsqueeze(0)\n",
    "    return input_tensor.to(DEVICE)    \n",
    "\n",
    "\n",
    "def preprocess_tensor(image):\n",
    "    \n",
    "    return image.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['n02690373', 'airliner']\n",
      "Class changed after 1 attacks.\n",
      "['n04033901', 'quill']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx=404\n",
    "def multiple_attacks(image, thresold=200):\n",
    "    attack_number = 0\n",
    "    img_tensor = preprocess_image(image)\n",
    "    img_tensor = img_tensor.requires_grad_(True)  # Ensure img_tensor is a leaf variable\n",
    "    assumption = True\n",
    "    scores = model(img_tensor)\n",
    "    my_pred = get_best_prediction(scores)\n",
    "    print(my_pred)\n",
    "    target = torch.tensor([idx]).to(DEVICE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    loss = criterion(scores, target)\n",
    "    loss.backward(retain_graph=True)  # Backpropagate the loss\n",
    "    data_grad = img_tensor.grad.data\n",
    "    \n",
    "\n",
    "    while attack_number < thresold and assumption:\n",
    "        \n",
    "        model.zero_grad()\n",
    "        img_tensor = compute_one_attack(img_tensor, data_grad)\n",
    "        attack_number += 1\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        img_tensor = img_tensor.requires_grad_(True)  # Ensure img_tensor is a leaf variable\n",
    "        target = torch.tensor([idx]).to(DEVICE)\n",
    "        scores = model(img_tensor)\n",
    "        loss = criterion(scores, target)\n",
    "        loss.backward(retain_graph=True)  # Backpropagate the loss\n",
    "        #data_grad = img_tensor.grad.data\n",
    "        new_best_prediction = get_best_prediction(scores)\n",
    "        #print(new_best_prediction)\n",
    "        if new_best_prediction == my_pred:\n",
    "            assumption = True\n",
    "        else:\n",
    "            assumption = False\n",
    "\n",
    "    print(\"Class changed after\", attack_number, \"attacks.\")\n",
    "    print( new_best_prediction)\n",
    "    return attack_number\n",
    "\n",
    "multiple_attacks(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
