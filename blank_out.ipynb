{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import numpy\n",
    "import sys\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from vit_rollout import VITAttentionRollout\n",
    "from vit_grad_rollout import VITAttentionGradRollout\n",
    "\n",
    "from tqdm import tqdm\n",
    "import scipy.stats"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for evaluating impact on precision by applying blank-out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import torch.nn.functional as F\n",
    "\n",
    "LABELS_URL = 'https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json'\n",
    "classes = {int(key): value for (key, value) in requests.get(LABELS_URL).json().items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n"
     ]
    }
   ],
   "source": [
    "img = Image.open(\"examples/input.png\")\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {DEVICE}\")\n",
    "\n",
    "IMAGE_SIZE = 224\n",
    "DISCARD_RATIO = 0.9\n",
    "\n",
    "def preprocess_image(image_path, transform):\n",
    "    img = Image.open(image_path)\n",
    "    input_tensor = transform(img).unsqueeze(0)\n",
    "    return input_tensor.to(DEVICE)\n",
    "\n",
    "def show_mask_on_image(img, mask):\n",
    "    img = np.float32(img) / 255\n",
    "    heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET)\n",
    "    heatmap = np.float32(heatmap) / 255\n",
    "    cam = heatmap + np.float32(img)\n",
    "    cam = cam / np.max(cam)\n",
    "    return np.uint8(255 * cam)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "])\n",
    "\n",
    "model = torch.hub.load('facebookresearch/deit:main', \n",
    "        'deit_tiny_patch16_224', pretrained=True)\n",
    "model.eval()\n",
    "model.to(DEVICE)\n",
    "print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor  = preprocess_image(\"examples/input.png\", transform)\n",
    "scores = model(input_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(scores):\n",
    "    '''Gets the index of max prob and the prob\n",
    "    '''\n",
    "    h_x = F.softmax(scores, dim=1).data.squeeze()\n",
    "    probs, idx = h_x.sort(0, True)\n",
    "    # output the prediction\n",
    "    return idx[0].item(), probs[0].item()\n",
    "\n",
    "idx, prob = get_prediction(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def compute_blankout_importance_conv(model, input_tensor, initial_class, patch_size=16, discard_ratio=0.9):\n",
    "    # Get the initial prediction\n",
    "    initial_scores = model(input_tensor)\n",
    "    initial_prob = F.softmax(initial_scores, dim=1)[0, initial_class].item()\n",
    "\n",
    "    # Create a deep copy of the input tensor\n",
    "    perturbed_tensor = copy.deepcopy(input_tensor)\n",
    "\n",
    "    # Initialize the importance vector\n",
    "    importance_vector = []\n",
    "\n",
    "    # Iterate through each patch in the input tensor\n",
    "    for i in range(0, input_tensor.size(2), patch_size):\n",
    "        for j in range(0, input_tensor.size(3), patch_size):\n",
    "            # Blank-out each patch in the input tensor\n",
    "            perturbed_tensor = copy.deepcopy(input_tensor)\n",
    "            perturbed_tensor[0, :, i:i+patch_size, j:j+patch_size] = 0  # Set all values to zero for each patch\n",
    "\n",
    "            # Get the prediction after blanking-out the patch\n",
    "            perturbed_scores = model(perturbed_tensor)\n",
    "            perturbed_prob = F.softmax(perturbed_scores, dim=1)[0, initial_class].item()\n",
    "\n",
    "            # Compute the impact on the probability\n",
    "            impact = initial_prob - perturbed_prob\n",
    "            importance_vector.append(impact)\n",
    "\n",
    "    return importance_vector\n",
    "\n",
    "# # Usage example:\n",
    "input_tensor = preprocess_image(\"examples/input.png\", transform)\n",
    "initial_class = get_prediction(scores)[0]\n",
    "importance_vector_direct = compute_blankout_importance_conv(model, input_tensor, initial_class, patch_size=16, discard_ratio=DISCARD_RATIO)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blank out pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_blank_out = 'blank_out.txt'\n",
    "\n",
    "with open(file_blank_out, 'w') as f:\n",
    "    pass  # Opening in 'w' mode clears the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_prefix = 'images/ILSVRC2012_val_00000'\n",
    "path_suffix = '.JPEG'\n",
    "discard_ratio = 0.9\n",
    "image_size = 224\n",
    "\n",
    "def convert_number(number):\n",
    "    if number < 10:\n",
    "        return '00'+str(number)\n",
    "    if number < 100:\n",
    "        return '0'+str(number)\n",
    "    else:\n",
    "        return str(number)\n",
    "\n",
    "# img = Image.open(path_prefix + image_number_converted + path_suffix)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/120 [00:00<?, ?it/s]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      "  1%|          | 1/120 [00:01<02:08,  1.08s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      "  2%|▏         | 2/120 [00:02<02:06,  1.08s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      "  2%|▎         | 3/120 [00:03<02:07,  1.09s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      "  3%|▎         | 4/120 [00:04<02:05,  1.08s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      "  4%|▍         | 5/120 [00:05<02:04,  1.08s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      "  5%|▌         | 6/120 [00:06<02:03,  1.08s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      "  6%|▌         | 7/120 [00:07<02:02,  1.08s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      "  7%|▋         | 8/120 [00:08<02:01,  1.09s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      "  8%|▊         | 9/120 [00:09<02:00,  1.08s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      "  8%|▊         | 10/120 [00:10<01:58,  1.08s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      "  9%|▉         | 11/120 [00:11<01:57,  1.08s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 10%|█         | 12/120 [00:12<01:57,  1.08s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 11%|█         | 13/120 [00:14<01:55,  1.08s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 12%|█▏        | 14/120 [00:15<01:54,  1.08s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 12%|█▎        | 15/120 [00:16<01:53,  1.08s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 13%|█▎        | 16/120 [00:17<01:52,  1.08s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 14%|█▍        | 17/120 [00:18<01:51,  1.08s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 15%|█▌        | 18/120 [00:19<01:50,  1.08s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 16%|█▌        | 19/120 [00:20<01:49,  1.09s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 17%|█▋        | 20/120 [00:21<01:48,  1.08s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 18%|█▊        | 21/120 [00:22<01:47,  1.09s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 18%|█▊        | 22/120 [00:23<01:46,  1.08s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 19%|█▉        | 23/120 [00:24<01:44,  1.08s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 20%|██        | 24/120 [00:25<01:43,  1.08s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 21%|██        | 25/120 [00:27<01:43,  1.09s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 22%|██▏       | 26/120 [00:28<01:42,  1.09s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 22%|██▎       | 27/120 [00:29<01:40,  1.09s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 23%|██▎       | 28/120 [00:30<01:42,  1.11s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 24%|██▍       | 29/120 [00:31<01:39,  1.09s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 25%|██▌       | 30/120 [00:32<01:37,  1.09s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 26%|██▌       | 31/120 [00:33<01:36,  1.08s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 27%|██▋       | 32/120 [00:34<01:34,  1.08s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 28%|██▊       | 33/120 [00:35<01:32,  1.06s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 29%|██▉       | 35/120 [00:36<01:09,  1.22it/s]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 30%|███       | 36/120 [00:37<01:14,  1.13it/s]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 31%|███       | 37/120 [00:38<01:17,  1.07it/s]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 32%|███▏      | 38/120 [00:39<01:19,  1.03it/s]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 32%|███▎      | 39/120 [00:41<01:20,  1.00it/s]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 33%|███▎      | 40/120 [00:42<01:21,  1.02s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 34%|███▍      | 41/120 [00:43<01:21,  1.04s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 35%|███▌      | 42/120 [00:44<01:22,  1.05s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 36%|███▌      | 43/120 [00:45<01:21,  1.06s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 37%|███▋      | 44/120 [00:46<01:20,  1.06s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 38%|███▊      | 45/120 [00:47<01:20,  1.07s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 38%|███▊      | 46/120 [00:48<01:18,  1.07s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 39%|███▉      | 47/120 [00:49<01:17,  1.06s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 40%|████      | 48/120 [00:50<01:15,  1.05s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 41%|████      | 49/120 [00:51<01:14,  1.05s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 42%|████▏     | 50/120 [00:52<01:13,  1.05s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 42%|████▎     | 51/120 [00:53<01:12,  1.05s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 43%|████▎     | 52/120 [00:54<01:11,  1.05s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 44%|████▍     | 53/120 [00:55<01:10,  1.05s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 45%|████▌     | 54/120 [00:56<01:09,  1.05s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 46%|████▌     | 55/120 [00:58<01:07,  1.05s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 47%|████▋     | 56/120 [00:59<01:07,  1.06s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 48%|████▊     | 57/120 [01:00<01:06,  1.06s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 48%|████▊     | 58/120 [01:01<01:05,  1.06s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 49%|████▉     | 59/120 [01:02<01:05,  1.08s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 50%|█████     | 60/120 [01:03<01:04,  1.07s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 51%|█████     | 61/120 [01:04<01:02,  1.07s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 52%|█████▏    | 62/120 [01:05<01:01,  1.06s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 52%|█████▎    | 63/120 [01:06<01:00,  1.06s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 53%|█████▎    | 64/120 [01:07<00:59,  1.07s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 54%|█████▍    | 65/120 [01:08<00:58,  1.06s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 55%|█████▌    | 66/120 [01:09<00:57,  1.06s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 56%|█████▌    | 67/120 [01:10<00:55,  1.05s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 57%|█████▋    | 68/120 [01:11<00:54,  1.05s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 57%|█████▊    | 69/120 [01:12<00:53,  1.05s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 58%|█████▊    | 70/120 [01:13<00:52,  1.05s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 59%|█████▉    | 71/120 [01:14<00:51,  1.05s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 60%|██████    | 72/120 [01:16<00:50,  1.06s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 61%|██████    | 73/120 [01:17<00:50,  1.08s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 62%|██████▏   | 74/120 [01:18<00:50,  1.09s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 62%|██████▎   | 75/120 [01:19<00:48,  1.09s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 63%|██████▎   | 76/120 [01:20<00:47,  1.08s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 64%|██████▍   | 77/120 [01:21<00:46,  1.08s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 65%|██████▌   | 78/120 [01:22<00:45,  1.07s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 66%|██████▌   | 79/120 [01:23<00:43,  1.07s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 67%|██████▋   | 80/120 [01:24<00:42,  1.07s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 68%|██████▊   | 81/120 [01:25<00:41,  1.07s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 68%|██████▊   | 82/120 [01:26<00:40,  1.07s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 69%|██████▉   | 83/120 [01:27<00:39,  1.06s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 70%|███████   | 84/120 [01:28<00:38,  1.07s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 71%|███████   | 85/120 [01:30<00:37,  1.06s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 72%|███████▏  | 86/120 [01:31<00:36,  1.07s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 72%|███████▎  | 87/120 [01:32<00:35,  1.06s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 73%|███████▎  | 88/120 [01:33<00:34,  1.06s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 74%|███████▍  | 89/120 [01:34<00:33,  1.09s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 75%|███████▌  | 90/120 [01:35<00:32,  1.08s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 76%|███████▌  | 91/120 [01:36<00:31,  1.07s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 77%|███████▋  | 92/120 [01:37<00:30,  1.07s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 78%|███████▊  | 93/120 [01:38<00:28,  1.07s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 78%|███████▊  | 94/120 [01:39<00:27,  1.07s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 79%|███████▉  | 95/120 [01:40<00:26,  1.07s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 80%|████████  | 96/120 [01:41<00:25,  1.07s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 81%|████████  | 97/120 [01:42<00:24,  1.06s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 82%|████████▏ | 98/120 [01:43<00:23,  1.06s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 82%|████████▎ | 99/120 [01:44<00:22,  1.06s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 83%|████████▎ | 100/120 [01:46<00:21,  1.06s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 84%|████████▍ | 101/120 [01:47<00:20,  1.06s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 85%|████████▌ | 102/120 [01:48<00:19,  1.06s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 86%|████████▌ | 103/120 [01:49<00:18,  1.06s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 87%|████████▋ | 104/120 [01:50<00:16,  1.06s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 88%|████████▊ | 105/120 [01:51<00:15,  1.06s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 88%|████████▊ | 106/120 [01:52<00:14,  1.06s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 90%|█████████ | 108/120 [01:53<00:09,  1.23it/s]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 91%|█████████ | 109/120 [01:54<00:09,  1.14it/s]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 92%|█████████▏| 110/120 [01:55<00:09,  1.08it/s]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 92%|█████████▎| 111/120 [01:56<00:08,  1.04it/s]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 93%|█████████▎| 112/120 [01:57<00:07,  1.01it/s]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 94%|█████████▍| 113/120 [01:58<00:07,  1.01s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 95%|█████████▌| 114/120 [01:59<00:06,  1.02s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 96%|█████████▌| 115/120 [02:00<00:05,  1.03s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 97%|█████████▋| 116/120 [02:01<00:04,  1.04s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 98%|█████████▊| 117/120 [02:02<00:03,  1.05s/it]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      " 99%|█████████▉| 119/120 [02:04<00:00,  1.23it/s]Using cache found in /users/eleves-a/2018/nicolas.lopes/.cache/torch/hub/facebookresearch_deit_main\n",
      "100%|██████████| 120/120 [02:05<00:00,  1.04s/it]\n"
     ]
    }
   ],
   "source": [
    "for image_number in tqdm(range(1,121)):\n",
    "    # print(f\"Current image being treated: {image_number}\")\n",
    "    if image_number in [34, 107, 118]:\n",
    "        with open(file_blank_out, 'a') as f:\n",
    "            f.write('\\n') \n",
    "    else:\n",
    "        image_number_converted = convert_number(image_number)\n",
    "        image_path = path_prefix + image_number_converted + path_suffix\n",
    "        input_tensor  = preprocess_image(image_path, transform)\n",
    "\n",
    "        # Getting idx\n",
    "        model = torch.hub.load('facebookresearch/deit:main', 'deit_tiny_patch16_224', pretrained=True)\n",
    "        model.eval()\n",
    "        model.to(DEVICE)\n",
    "        scores = model(input_tensor)\n",
    "        category_index, _ = get_prediction(scores)\n",
    "\n",
    "        initial_class = get_prediction(scores)[0]\n",
    "        importance_vector_direct = compute_blankout_importance_conv(model, input_tensor, category_index, patch_size=16, discard_ratio=DISCARD_RATIO)\n",
    "        importance_vector_direct = scipy.stats.rankdata(importance_vector_direct, method='average')\n",
    "        \n",
    "        with open(file_blank_out, 'a') as f:\n",
    "            np.savetxt(f, [importance_vector_direct], fmt='%.3f', delimiter=',') \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your original list of values\n",
    "values = [-0.5, 3, 9, 8, 2, 7, 10, 1, 4, 6]  # For example\n",
    "\n",
    "# Get the Spearman ranks of the values\n",
    "ranks = scipy.stats.rankdata(values, method='average')  # 'average' deals with ties by assigning the average rank\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>186</th>\n",
       "      <th>187</th>\n",
       "      <th>188</th>\n",
       "      <th>189</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.091</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.088</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.088</td>\n",
       "      <td>...</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.452</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.737</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.083</td>\n",
       "      <td>...</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.318</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.385</td>\n",
       "      <td>0.747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.030</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.066</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.074</td>\n",
       "      <td>...</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.197</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.038</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.036</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>0.073</td>\n",
       "      <td>0.787</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.466</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.072</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>0.032</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.028</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>0.067</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.070</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.754</td>\n",
       "      <td>0.799</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.444</td>\n",
       "      <td>0.190</td>\n",
       "      <td>...</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>0.047</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>...</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>0.028</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.027</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.487</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.027</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>117 rows × 196 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0      1      2      3      4      5      6      7      8      9    \\\n",
       "0    0.091  0.086  0.087  0.088  1.000  0.086  0.089  0.086  0.086  0.088   \n",
       "1    0.737  0.076  0.079  0.074  0.093  0.088  0.085  0.082  0.084  0.083   \n",
       "2    1.000  0.031  0.030  0.030  0.030  0.030  0.031  0.030  0.030  0.030   \n",
       "3    0.066  0.066  0.067  0.066  0.065  0.064  0.065  0.066  0.065  0.074   \n",
       "4    0.038  0.037  0.037  0.037  0.036  0.037  0.037  0.037  0.036  0.036   \n",
       "..     ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "112  0.073  0.787  0.072  0.466  0.073  0.072  0.076  0.160  0.074  0.072   \n",
       "113  0.032  0.032  0.029  0.029  0.029  0.029  0.092  0.255  0.029  0.028   \n",
       "114  0.067  0.068  0.070  1.000  0.754  0.799  0.068  0.067  0.444  0.190   \n",
       "115  0.047  0.044  0.046  0.044  0.045  0.044  0.045  0.045  0.045  0.045   \n",
       "116  0.028  0.068  0.027  0.027  0.028  0.027  0.027  0.027  0.027  0.027   \n",
       "\n",
       "     ...    186    187    188    189    190    191    192    193    194    195  \n",
       "0    ...  0.087  0.090  0.452  0.091  0.090  0.083  0.090  0.091  0.089  0.088  \n",
       "1    ...  0.087  0.077  0.084  0.086  0.083  0.076  0.318  0.076  0.385  0.747  \n",
       "2    ...  0.030  0.030  0.030  0.030  0.030  0.030  0.030  0.031  0.030  0.030  \n",
       "3    ...  0.135  0.197  0.066  0.067  0.062  0.121  0.063  0.062  0.065  0.070  \n",
       "4    ...  0.046  0.037  0.038  0.037  0.038  0.038  0.038  0.041  0.038  0.038  \n",
       "..   ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...  \n",
       "112  ...  0.071  0.069  0.072  0.072  0.072  0.072  0.070  0.071  0.069  0.071  \n",
       "113  ...  0.029  0.029  0.030  0.030  0.030  0.030  0.030  0.029  0.029  0.031  \n",
       "114  ...  0.068  0.070  0.068  0.064  0.064  0.066  0.065  0.064  0.063  0.065  \n",
       "115  ...  0.600  0.047  0.047  0.045  0.046  0.045  0.045  0.044  0.045  0.047  \n",
       "116  ...  0.028  0.027  0.487  0.027  0.027  0.027  0.027  0.027  0.027  0.027  \n",
       "\n",
       "[117 rows x 196 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('images_done/attention_flow.txt', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "      <th>255</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.037</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.071</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.025</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>0.027</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.071</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.068</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.047</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>-0.121</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.006</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.004</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>-0.111</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.134</td>\n",
       "      <td>-0.140</td>\n",
       "      <td>-0.139</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>0.088</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.051</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.045</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>-0.016</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.015</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.010</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>0.004</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>0.007</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.004</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>117 rows × 256 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0      1      2      3      4      5      6      7      8      9    \\\n",
       "0   -0.037 -0.023 -0.035  0.002 -0.020  0.010 -0.014 -0.016 -0.028 -0.016   \n",
       "1   -0.071 -0.003 -0.040  0.017  0.039  0.068  0.025 -0.027 -0.033  0.027   \n",
       "2   -0.071 -0.013 -0.066 -0.016 -0.003 -0.022 -0.013 -0.068 -0.015 -0.026   \n",
       "3    0.003 -0.020 -0.010 -0.004 -0.010 -0.012  0.009 -0.004 -0.003  0.003   \n",
       "4   -0.059 -0.027 -0.036 -0.010  0.008 -0.001 -0.011 -0.048 -0.014 -0.016   \n",
       "..     ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "112 -0.111 -0.014 -0.027 -0.059 -0.031 -0.023 -0.053 -0.034 -0.042 -0.081   \n",
       "113 -0.077 -0.001  0.021  0.001 -0.039 -0.052 -0.046 -0.114 -0.053 -0.040   \n",
       "114  0.088  0.019  0.102  0.014  0.064  0.085  0.056  0.006  0.051 -0.001   \n",
       "115 -0.016 -0.005  0.015 -0.000 -0.001  0.013  0.016  0.010  0.002  0.010   \n",
       "116  0.007  0.009  0.015  0.008  0.003  0.004 -0.000 -0.010 -0.013 -0.007   \n",
       "\n",
       "     ...    246    247    248    249    250    251    252    253    254    255  \n",
       "0    ... -0.034 -0.024 -0.025 -0.021 -0.039 -0.027 -0.021 -0.017 -0.008 -0.031  \n",
       "1    ...  0.002 -0.018 -0.017 -0.002 -0.079 -0.023 -0.029 -0.024 -0.010 -0.023  \n",
       "2    ... -0.047 -0.003 -0.032 -0.005 -0.022 -0.003  0.010  0.015  0.002  0.015  \n",
       "3    ... -0.099 -0.034 -0.041 -0.121 -0.099 -0.063 -0.065 -0.030  0.001  0.005  \n",
       "4    ...  0.019  0.006 -0.023  0.009 -0.005  0.014  0.004 -0.021 -0.002 -0.010  \n",
       "..   ...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...  \n",
       "112  ...  0.004  0.008  0.010  0.007  0.001 -0.003 -0.004 -0.001  0.005  0.003  \n",
       "113  ... -0.075 -0.134 -0.140 -0.139 -0.133 -0.107 -0.031 -0.039  0.007 -0.136  \n",
       "114  ...  0.012  0.003  0.045 -0.014  0.106  0.044  0.014  0.002 -0.002  0.031  \n",
       "115  ... -0.007 -0.006  0.009  0.009 -0.007 -0.019 -0.036  0.004 -0.009 -0.001  \n",
       "116  ...  0.008 -0.005 -0.006  0.015  0.002 -0.003 -0.007  0.007  0.013  0.002  \n",
       "\n",
       "[117 rows x 256 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('blank_out.txt', header = None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTANT\n",
    "\n",
    "\n",
    "MUST CHECK IF THE CORRESPONDANCE OF EMBEDDING IS THE SAME AS IN THE BLANK OUT !!!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
